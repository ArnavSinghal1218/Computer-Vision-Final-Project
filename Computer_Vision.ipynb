{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArnavSinghal1218/Computer-Vision-Final-Project/blob/main/Computer_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Emotion detection for facial expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-Q8Xl3y63IP_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x219451c0070>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset,DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('fer2013.csv')\n",
        "\n",
        "class FER2013Dataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Die Pixel-Spalte und Label-Spalte aus der CSV\n",
        "        pixels = self.dataframe.iloc[idx, 1]\n",
        "        label = int(self.dataframe.iloc[idx, 0])\n",
        "\n",
        "        # Pixelwerte in 48x48 Bild umwandeln\n",
        "        image = np.fromstring(pixels, sep=' ').reshape(48, 48).astype(np.uint8)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Transformationen definieren\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((48, 48)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Dataset erstellen\n",
        "fer_dataset = FER2013Dataset(dataframe=df, transform=transform)\n",
        "\n",
        "# Split in Training (80%), Validation (10%) und Test (10%) setzen\n",
        "train_size = int(0.8 * len(fer_dataset))\n",
        "val_size = int(0.1 * len(fer_dataset))\n",
        "test_size = len(fer_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(fer_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# DataLoader erstellen\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bilder: torch.Size([32, 1, 48, 48])\n",
            "Labels: tensor([3, 6, 4, 3, 3, 5, 3, 0, 2, 4, 3, 3, 5, 2, 0, 0, 0, 6, 0, 0, 0, 4, 6, 4,\n",
            "        6, 6, 3, 0, 6, 3, 5, 5])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Beispiel: Lade eine Batch aus dem Trainings-Loader\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "print(f\"Bilder: {images.shape}\")\n",
        "print(f\"Labels: {labels}\")\n",
        "\n",
        "# Zeige ein Bild an\n",
        "plt.imshow(images[0].squeeze(), cmap=\"gray\")\n",
        "plt.title(f\"Label: {labels[0].item()}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 1. Convolutional Block (Conv -> BatchNorm -> ReLU -> MaxPool)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # 2. Convolutional Block\n",
        "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, padding=1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(256 * 3 * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 7)  # 7 Klassen für Emotionen\n",
        "        \n",
        "        # Dropout für Regularisierung\n",
        "        #self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Convolutional Block\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        \n",
        "        # 2. Convolutional Block\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        \n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(-1, 256 * 3 * 3)  # Bildgröße 48x48 wird durch Pooling auf 3x3 reduziert\n",
        "        \n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Dropout zur Regularisierung\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.C=10\n",
        "        # Convolution layers\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=(5,5), padding=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=(10,10))\n",
        "        # Fully connected layers\n",
        "        self.fc1= nn.Linear(784, 240)\n",
        "        self.fc2 = nn.Linear(240, 120)\n",
        "        self.fc3=nn.Linear(120,self.C+5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, C_in, H, W)\n",
        "        out=self.conv1(x)\n",
        "        out=F.relu(out)\n",
        "        out=F.avg_pool2d(out,kernel_size=2,stride=2)\n",
        "        out=self.conv2(out)\n",
        "        out=F.relu(out)\n",
        "        out=F.avg_pool2d(out,kernel_size=2,stride=2)\n",
        "        \n",
        "        #From now on the FC layers\n",
        "        out = out.view(-1, out.shape[-3]*out.shape[-2]*out.shape[-1])\n",
        "        \n",
        "        out =self.fc1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VGGNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 1. Convolutional Block (Conv -> BatchNorm -> ReLU -> MaxPool)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        # 2. Convolutional Block\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        # 3. Convolutional Block\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        # 4. Convolutional Block\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(256 * 3 * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 7)  # 7 Klassen für Emotionen\n",
        "        \n",
        "        # Dropout für Regularisierung\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Convolutional Block\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        \n",
        "        # 2. Convolutional Block\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        \n",
        "        # 3. Convolutional Block\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # 4. Convolutional Block\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(-1, 256 * 3 * 3)  # Bildgröße 48x48 wird durch Pooling auf 3x3 reduziert\n",
        "        \n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Dropout zur Regularisierung\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer12 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer13 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer9(out)\n",
        "        out = self.layer10(out)\n",
        "        out = self.layer11(out)\n",
        "        out = self.layer12(out)\n",
        "        out = self.layer13(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VGG16light(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(VGG16light, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(18432, 1024),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(1024, num_classes))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trainings- und Validierungsschleife\n",
        "def train(model, train_loader, val_loader, epochs,optimizer,scheduler):\n",
        "    model.train()  # Setze den Modus auf Training\n",
        "    optimizer.zero_grad(set_to_none=True)  # Gradienten zurücksetzen\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Training\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            \n",
        "            outputs = model(images)  # Vorwärtsdurchlauf\n",
        "            loss = criterion(outputs, labels)  # Berechne den Verlust\n",
        "            loss.backward()  # Rückwärtsdurchlauf\n",
        "            optimizer.step()  # Parameter aktualisieren\n",
        "            optimizer.zero_grad()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Nach jeder Epoche Validierung durchführen\n",
        "        val_loss, val_accuracy = validate(model, val_loader)\n",
        "        scheduler.step(val_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "def cosine_accuracy(outputs, labels):\n",
        "    # 1. Labels in One-Hot Encoding umwandeln\n",
        "    labels_one_hot = F.one_hot(labels, num_classes=outputs.size(1)).float()\n",
        "    \n",
        "    # 2. Cosine Similarity zwischen den Vorhersagen (outputs) und den One-Hot-Labels berechnen\n",
        "    cosine_sim = F.cosine_similarity(outputs, labels_one_hot, dim=1)\n",
        "    \n",
        "    # 3. Als korrekt zählen, wenn der Cosine Similarity-Wert > 0.5 ist (kann angepasst werden)\n",
        "    correct = torch.sum(cosine_sim > 0.5).item()\n",
        "    \n",
        "    # 4. Gesamtzahl der Beispiele\n",
        "    total = labels.size(0)\n",
        "    \n",
        "    # 5. Cosine Accuracy berechnen\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "# Validierungsfunktion\n",
        "def validate(model, val_loader):\n",
        "    model.eval()  # Setze den Modus auf Evaluation (kein Dropout oder BatchNorm)\n",
        "    val_loss = 0.0\n",
        "    cosine_acc = 0.0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Deaktiviere den Gradienten\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Berechne Cosine Similarity Accuracy\n",
        "            cosine_acc += cosine_accuracy(outputs, labels)\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return val_loss / len(val_loader), cosine_acc / len(val_loader)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_selection_and_evaluation(models, train_loader, val_loader,n_epochs):\n",
        "    best_model = None\n",
        "    best_accuracy = 0\n",
        "    results = []\n",
        "\n",
        "    for i, (model_name, model) in enumerate(models.items()):\n",
        "        print(f\"\\nTraining {model_name}...\")\n",
        "        model = model.to(device)\n",
        "        \n",
        "        \n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01,nesterov=True,momentum=0.9,weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.75, patience=5, verbose=True)\n",
        "        # Modell trainieren\n",
        "        train(model, train_loader,val_loader, n_epochs, optimizer,scheduler)\n",
        "        \n",
        "        # Modell evaluieren\n",
        "        _,accuracy =validate(model, val_loader)\n",
        "        print(f\"{model_name} Accuracy: {accuracy:.2f}%\")\n",
        "        results.append((model_name, accuracy))\n",
        "\n",
        "        # Bestes Modell auswählen\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_model = model\n",
        "    \n",
        "    print(\"\\nModellvergleich:\")\n",
        "    for model_name, accuracy in results:\n",
        "        print(f\"{model_name}: {accuracy:.2f}%\")\n",
        "    \n",
        "    return best_model, best_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on device cpu.\n",
            "\n",
            "Training VGGNet16...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arnel\\pyver\\py3123\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 12\u001b[0m\n\u001b[0;32m      3\u001b[0m n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      4\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVGGNet16\u001b[39m\u001b[38;5;124m\"\u001b[39m:VGG16light(),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVGGNet\u001b[39m\u001b[38;5;124m\"\u001b[39m: VGGNet(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \n\u001b[0;32m     10\u001b[0m }\n\u001b[1;32m---> 12\u001b[0m best_model, best_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_selection_and_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m _,test_accuracy\u001b[38;5;241m=\u001b[39mvalidate(best_model, test_loader)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBestes Modell: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m mit einer test- Genauigkeit von \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[32], line 14\u001b[0m, in \u001b[0;36mmodel_selection_and_evaluation\u001b[1;34m(models, train_loader, val_loader, n_epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Modell trainieren\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Modell evaluieren\u001b[39;00m\n\u001b[0;32m     17\u001b[0m _,accuracy \u001b[38;5;241m=\u001b[39mvalidate(model, val_loader)\n",
            "Cell \u001b[1;32mIn[31], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, epochs, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)  \u001b[38;5;66;03m# Vorwärtsdurchlauf\u001b[39;00m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Berechne den Verlust\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Rückwärtsdurchlauf\u001b[39;00m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Parameter aktualisieren\u001b[39;00m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\arnel\\pyver\\py3123\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\arnel\\pyver\\py3123\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\arnel\\pyver\\py3123\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on device {device}.\")\n",
        "n_epochs=5\n",
        "models = {\n",
        "    \"VGGNet16\":VGG16light(),\n",
        "    \"VGGNet\": VGGNet(),\n",
        "    \"LeNet5\": LeNet5()\n",
        "    \n",
        "    \n",
        "}\n",
        "\n",
        "best_model, best_accuracy = model_selection_and_evaluation(models, train_loader, val_loader,n_epochs)\n",
        "_,test_accuracy=validate(best_model, test_loader)\n",
        "print(f\"\\nBestes Modell: {best_model.__class__.__name__} mit einer test- Genauigkeit von {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNCNc2OWKMLHH7jGCWgIaDE",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
