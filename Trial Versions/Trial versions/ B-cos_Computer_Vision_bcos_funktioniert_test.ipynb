{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNmmAd1QVBYM"
      },
      "source": [
        "# Emotion detection for facial expressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q8Xl3y63IP_",
        "outputId": "95896ada-a92f-4c6e-aaed-3ee7405a4379"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "torch.manual_seed(123)\n",
        "from IPython.display import clear_output\n",
        "from typing import cast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loj2njOIVBYV"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FToH7__xVBYW"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('fer2013.csv')\n",
        "\n",
        "class FER2013Dataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #Load pixels and labels from csv\n",
        "        pixels = self.dataframe.iloc[idx, 1]\n",
        "        label = int(self.dataframe.iloc[idx, 0])\n",
        "\n",
        "        # Convert to 48x48 pixel image\n",
        "        image = np.fromstring(pixels, sep=' ').reshape(48, 48).astype(np.uint8)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((48, 48)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "fer_dataset = FER2013Dataset(dataframe=df, transform=transform)\n",
        "\n",
        "# Split in Training (80%), Validation (10%) and Test (10%)\n",
        "train_size = int(0.8 * len(fer_dataset))\n",
        "val_size = int(0.1 * len(fer_dataset))\n",
        "test_size = len(fer_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(fer_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufTMT-NJVBYY"
      },
      "source": [
        "## Implementing the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class NormedConv2d(nn.Conv2d):\n",
        "    \"\"\"\n",
        "    Standard 2D convolution, but with unit norm weights.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, in_tensor):\n",
        "        shape = self.weight.shape\n",
        "        w = self.weight.view(shape[0], -1)\n",
        "        w = w/(w.norm(p=2, dim=1, keepdim=True))\n",
        "        return F.conv2d(in_tensor, w.view(shape),\n",
        "                        self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "class BcosConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, inc, outc, kernel_size=1, stride=1, padding=0, max_out=2, b=2,\n",
        "                 scale=None, scale_fact=100, **kwargs):\n",
        "        super().__init__()\n",
        "        \n",
        "        ks = kernel_size\n",
        "        self.stride = stride\n",
        "        self.linear = NormedConv2d(inc, outc * max_out, ks, stride, padding, 1, 1, bias=False)\n",
        "        self.outc = outc * max_out\n",
        "        self.b = b\n",
        "        self.max_out = max_out\n",
        "        self.inc = inc\n",
        "        self.kernel_size = ks\n",
        "        self.kssq = ks**2 if not isinstance(ks, tuple) else np.prod(ks)\n",
        "        self.padding = padding\n",
        "        self.detach = False\n",
        "        if scale is None:\n",
        "            ks_scale = ks if not isinstance(ks, tuple) else np.sqrt(np.prod(ks))\n",
        "            self.scale = (ks_scale * np.sqrt(self.inc)) / scale_fact\n",
        "        else:\n",
        "            self.scale = scale\n",
        "\n",
        "    def forward(self, in_tensor):\n",
        "        \"\"\"\n",
        "        In the case of B=2, we do not have to explicitly calculate the cosine term.\n",
        "        Args:\n",
        "            in_tensor: Input tensor. Expected shape: (B, C, H, W)\n",
        "\n",
        "        Returns:\n",
        "            BcosConv2d output on the input tensor.\n",
        "        \"\"\"\n",
        "        if self.b == 2:\n",
        "            return self.fwd_2(in_tensor)\n",
        "        return self.fwd_b(in_tensor)\n",
        "\n",
        "    def explanation_mode(self, detach=True):\n",
        "        \"\"\"\n",
        "        Enter 'explanation mode' by setting self.explain and self.detach.\n",
        "        Args:\n",
        "            detach: Whether to 'detach' the weight matrix from the computational graph so that it is not\n",
        "                            taken into account in the backward pass.\n",
        "\n",
        "        Returns: None\n",
        "\n",
        "        \"\"\"\n",
        "        self.detach = detach\n",
        "\n",
        "    def fwd_b(self, in_tensor):\n",
        "        # Simple linear layer\n",
        "        out = self.linear(in_tensor)\n",
        "        bs, _, h, w = out.shape\n",
        "\n",
        "        # MaxOut computation\n",
        "        if self.max_out > 1:\n",
        "            bs, _, h, w = out.shape\n",
        "            out = out.view(bs, -1, self.max_out, h, w)\n",
        "            out = out.max(dim=2, keepdim=False)[0]\n",
        "\n",
        "        # If B=1, no further calculation necessary.\n",
        "        if self.b == 1:\n",
        "            return out / self.scale\n",
        "\n",
        "        # Calculating the norm of input patches. Use average pooling and upscale by kernel size.\n",
        "        norm = (F.avg_pool2d((in_tensor ** 2).sum(1, keepdim=True), self.kernel_size, padding=self.padding,\n",
        "                             stride=self.stride) * self.kssq + 1e-6  # stabilising term\n",
        "                ).sqrt_()\n",
        "\n",
        "        # get absolute value of cos\n",
        "        # TODO: unnecessary doubling of stabilising term. Only affects CIFAR10 experiments in the paper.\n",
        "        abs_cos = (out / norm).abs() + 1e-6\n",
        "\n",
        "        # In order to compute the explanations, we detach the dynamically calculated scaling from the graph.\n",
        "        if self.detach:\n",
        "            abs_cos = abs_cos.detach()\n",
        "\n",
        "        # additional factor of cos^(b-1) s.t. in total we have norm * cos^b with original sign\n",
        "        out = out * abs_cos.pow(self.b-1)\n",
        "        return out / self.scale\n",
        "\n",
        "    def fwd_2(self, in_tensor):\n",
        "        # Simple linear layer\n",
        "        out = self.linear(in_tensor)\n",
        "\n",
        "        # MaxOut computation\n",
        "        if self.max_out > 1:\n",
        "            bs, _, h, w = out.shape\n",
        "            out = out.view(bs, -1, self.max_out, h, w)\n",
        "            out = out.max(dim=2, keepdim=False)[0]\n",
        "\n",
        "        # Calculating the norm of input patches. Use average pooling and upscale by kernel size.\n",
        "        # TODO: implement directly as F.sum_pool2d...\n",
        "        norm = (F.avg_pool2d((in_tensor ** 2).sum(1, keepdim=True), self.kernel_size, padding=self.padding,\n",
        "                                    stride=self.stride) * self.kssq + 1e-6  # stabilising term\n",
        "                ).sqrt_()\n",
        "\n",
        "        # In order to compute the explanations, we detach the dynamically calculated scaling from the graph.\n",
        "        if self.detach:\n",
        "            out = (out * out.abs().detach())\n",
        "            norm = norm.detach()\n",
        "        else:\n",
        "            out = (out * out.abs())\n",
        "\n",
        "        return out / (norm * self.scale)\n",
        "\n",
        "                \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_layers() -> nn.Sequential:\n",
        "    layers: List[nn.Module] = []\n",
        "    in_channels = 1\n",
        "    #cfg=[64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']#VGG11 Architec\n",
        "    cfg=[16, 'M', 32, 'M', 64,'M',64, 'M']\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.AvgPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            v=cast(int,v)\n",
        "            conv2d = BcosConv2d(in_channels, v, kernel_size=3, padding=1, stride=1, scale_fact=1000)\n",
        "            layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "    ) -> None:\n",
        "        super(VGG, self).__init__()\n",
        "        self.num_classes:int=7\n",
        "        self.features = make_layers()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            BcosConv2d(64, 1024, kernel_size=7, padding=3, scale_fact=1000),\n",
        "            BcosConv2d(1024, 1024, scale_fact=1000),\n",
        "            BcosConv2d(1024, self.num_classes, scale_fact=1000),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def get_features(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "    def get_sequential_model(self):\n",
        "        model = nn.Sequential(\n",
        "            *[m for m in self.features], self.classifier\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def get_layer_idx(self, idx):\n",
        "        return int(np.ceil(len(self.get_sequential_model())*idx/10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "jYMIRhcq1lad"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Cosine similarity-based loss function\n",
        "def cosine_loss(outputs, labels):\n",
        "    labels_one_hot = F.one_hot(labels, num_classes=outputs.size(1)).float()\n",
        "    cosine_sim = F.cosine_similarity(outputs, labels_one_hot, dim=1)\n",
        "    loss = 1 - torch.mean(cosine_sim)  # Use cosine similarity as loss\n",
        "    return loss\n",
        "\n",
        "# Validation function with cosine accuracy\n",
        "def validate(model, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    cosine_acc = 0.0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = cosine_loss(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            cosine_acc += cosine_accuracy(outputs, labels)\n",
        "            total += labels.size(0)\n",
        "    return val_loss / len(val_loader), cosine_acc / len(val_loader)\n",
        "\n",
        "# Cosine accuracy calculation\n",
        "def cosine_accuracy(outputs, labels):\n",
        "    labels_one_hot = F.one_hot(labels, num_classes=outputs.size(1)).float()\n",
        "    cosine_sim = F.cosine_similarity(outputs, labels_one_hot, dim=1)\n",
        "    correct = torch.sum(cosine_sim > 0.5).item()\n",
        "    total = labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdoq4Z_AVBYc"
      },
      "source": [
        "## Define training and validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S0M_sc5VBYe"
      },
      "source": [
        "## Training the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "0SQD1f0-VBYe"
      },
      "outputs": [],
      "source": [
        "def model_selection_and_evaluation(models, train_loader, val_loader,n_epochs):\n",
        "    plt.ion()  # Interaktiver Modus aktivieren\n",
        "\n",
        "    best_model = None\n",
        "    best_accuracy = 0\n",
        "    results = []\n",
        "    for epoch in range(n_epochs):\n",
        "        for i, (model_name, model) in enumerate(models.items()):\n",
        "            print(f\"\\nTraining {model_name}...\")\n",
        "            model = model.to(device)\n",
        "        \n",
        "        \n",
        "            optimizer = optim.SGD(model.parameters(), lr=0.01,nesterov=True,momentum=0.9,weight_decay=1e-4)\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.75, patience=5, verbose=True)\n",
        "            # Train model\n",
        "            model.train()  \n",
        "            optimizer.zero_grad(set_to_none=True)  #set gradients to zero\n",
        "        \n",
        "        \n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Training\n",
        "            for images, labels in train_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            \n",
        "                outputs= model(images)  # Foreward pass\n",
        "                print(outputs.shape)\n",
        "                print(labels.shape)\n",
        "                loss = nn.CrossEntropyLoss(outputs, labels)  # Calculate loss, muss noch durch cosine loss ersetzt werden\n",
        "                loss.backward()  # Back pass\n",
        "                optimizer.step()  # update parameters\n",
        "                optimizer.zero_grad()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Calculate loss and accuracy for the validation data\n",
        "            val_loss, val_accuracy = validate(model, val_loader)\n",
        "            scheduler.step(val_loss)\n",
        "            print(f\"Epoch [{epoch+1}/{n_epochs}],model name {model_name} Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "        \n",
        "            # Evaluate model\n",
        "\n",
        "            results.append((model_name, val_accuracy))\n",
        "\n",
        "            # Select best model\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_accuracy = val_accuracy\n",
        "                best_model = model\n",
        "        \n",
        "\n",
        "        model_dict = {}\n",
        "        for model, accuracy in results:\n",
        "            if model not in model_dict:\n",
        "                model_dict[model] = []  # Create a list for each model\n",
        "            model_dict[model].append(accuracy)\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "    \n",
        "        clear_output(wait=True)\n",
        "        for model, accuracies in model_dict.items():\n",
        "            plt.plot(range(1, len(accuracies) + 1), accuracies, marker='o', label=model)\n",
        "        \n",
        "        # Achsen und Titel\n",
        "        plt.title('Validation accuracy as a function of number of epochs for all models')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.show()\n",
        "        plt.savefig(\"accuracy.png\")\n",
        "    print(\"\\nModell comparison:\")\n",
        "    for model_name, accuracy in results:\n",
        "        print(f\"{model_name}: {accuracy:.2f}%\")\n",
        "    plt.ioff()  # Interaktiven Modus deaktivieren\n",
        "    plt.show()  # Plot am Ende anzeigen\n",
        "    return best_model, best_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "G619yVGwVBYe",
        "outputId": "a4c4b8be-54e7-4b61-ff3f-06b7780bf481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on device cpu.\n",
            "\n",
            "Training Bcos LeNet5:...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\arnel\\pyver\\py3123\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 7, 3, 3])\n",
            "torch.Size([32])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Boolean value of Tensor with more than one value is ambiguous",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[74], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Put in here the models you want to train\u001b[39;00m\n\u001b[0;32m      5\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBcos LeNet5:\u001b[39m\u001b[38;5;124m\"\u001b[39m:VGG(),\n\u001b[0;32m      7\u001b[0m    \n\u001b[0;32m      8\u001b[0m }\n\u001b[1;32m---> 10\u001b[0m best_model, best_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_selection_and_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m _,test_accuracy\u001b[38;5;241m=\u001b[39mvalidate(best_model, test_loader)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100.\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with a test accuracy of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[73], line 30\u001b[0m, in \u001b[0;36mmodel_selection_and_evaluation\u001b[1;34m(models, train_loader, val_loader, n_epochs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 30\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calculate loss, muss noch durch cosine loss ersetzt werden\u001b[39;00m\n\u001b[0;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Back pass\u001b[39;00m\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# update parameters\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\arnel\\pyver\\py3123\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1183\u001b[0m, in \u001b[0;36mCrossEntropyLoss.__init__\u001b[1;34m(self, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m   1182\u001b[0m              reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, label_smoothing: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1183\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index \u001b[38;5;241m=\u001b[39m ignore_index\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing \u001b[38;5;241m=\u001b[39m label_smoothing\n",
            "File \u001b[1;32mc:\\Users\\arnel\\pyver\\py3123\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:30\u001b[0m, in \u001b[0;36m_WeightedLoss.__init__\u001b[1;34m(self, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m, weight)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight: Optional[Tensor]\n",
            "File \u001b[1;32mc:\\Users\\arnel\\pyver\\py3123\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:23\u001b[0m, in \u001b[0;36m_Loss.__init__\u001b[1;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy_get_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m=\u001b[39m reduction\n",
            "File \u001b[1;32mc:\\Users\\arnel\\pyver\\py3123\\Lib\\site-packages\\torch\\nn\\_reduction.py:35\u001b[0m, in \u001b[0;36mlegacy_get_string\u001b[1;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     reduce \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mand\u001b[39;00m reduce:\n\u001b[0;32m     36\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on device {device}.\")\n",
        "n_epochs=20\n",
        "#Put in here the models you want to train\n",
        "models = {\n",
        "   \"Bcos LeNet5:\":VGG(),\n",
        "   \n",
        "}\n",
        "\n",
        "best_model, best_accuracy = model_selection_and_evaluation(models, train_loader, val_loader,n_epochs)\n",
        "_,test_accuracy=validate(best_model, test_loader)*100.\n",
        "print(f\"\\nBest model: {best_model.__class__.__name__} with a test accuracy of {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bilder: torch.Size([32, 1, 48, 48])\n",
            "Labels: tensor([4, 5, 2, 3, 3, 3, 3, 0, 2, 3, 5, 3, 6, 5, 6, 2, 3, 6, 6, 6, 0, 3, 0, 3,\n",
            "        4, 3, 0, 3, 3, 2, 6, 1])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Example: Load a batch and plot an image\n",
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "print(f\"Bilder: {images.shape}\")\n",
        "print(f\"Labels: {labels}\")\n",
        "\n",
        "plt.imshow(images[0].squeeze(), cmap=\"gray\")\n",
        "plt.title(f\"Label: {labels[0].item()}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "dic_label={0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "emotion=dic_label[int(best_model(images[0]).argmax())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "MwJHG8b2VBYf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 120)"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img=models[\"Bcos LeNet5:\"].cos_sim1\n",
        "img=img.detach().numpy()\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
